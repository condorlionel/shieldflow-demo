import streamlit as st
import time
import re
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import Optional

# --- 1. CONFIGURATION ---
st.set_page_config(page_title="ShieldFlow Hybrid Core", page_icon="üõ°Ô∏è", layout="wide")

# --- 2. MOTEUR HYBRIDE (FONCTIONS DE PERFORMANCE) ---

# NIVEAU 1 : Validation Regex (Ultra-Rapide < 1ms)
def quick_validate_email(text: str) -> bool:
    """V√©rifie s'il y a au moins un semblant d'email dans le texte."""
    # Regex simple : quelque chose @ quelque chose . quelque chose
    match = re.search(r"[^@]+@[^@]+\.[^@]+", text)
    return bool(match)

# NIVEAU 2 : Cache (Simulation Redis avec Session State)
if "cache_db" not in st.session_state:
    st.session_state["cache_db"] = {}

def check_cache(raw_text: str):
    """V√©rifie si on a d√©j√† trait√© cette demande exacte."""
    return st.session_state["cache_db"].get(raw_text)

def save_to_cache(raw_text: str, result_data: dict):
    """Sauvegarde le r√©sultat pour la prochaine fois."""
    st.session_state["cache_db"][raw_text] = result_data

# --- 3. MOD√àLE DE DONN√âES ---
class CleanedContact(BaseModel):
    full_name: Optional[str] = Field(description="Pr√©nom et Nom corrig√©s")
    email: Optional[str] = Field(description="Email valide")
    job_title: Optional[str] = Field(description="Titre du poste original")
    standardized_role: Optional[str] = Field(description="R√¥le standardis√© (ex: CEO, Sales)")
    company_name: Optional[str] = Field(description="Nom de l'entreprise")
    company_industry: Optional[str] = Field(description="Secteur d'activit√©")
    risk_flag: bool = Field(description="Vrai si risqu√©")
    risk_reason: Optional[str] = Field(description="Raison du risque")
    processing_source: str = Field(description="Source du traitement: 'CACHE' ou 'AI'")

# --- 4. INTERFACE ---
st.title("üõ°Ô∏è ShieldFlow Core")
st.caption("Architecture Hybride : Regex -> Cache -> IA")

# Gestion Cl√© API
api_key = None
if "OPENAI_API_KEY" in st.secrets:
    api_key = st.secrets["OPENAI_API_KEY"]
else:
    api_key = st.sidebar.text_input("Cl√© API OpenAI", type="password")

if not api_key:
    st.warning("Entrez une cl√© API pour activer le Niveau 3 (IA).")
    st.stop()

# Initialisation IA (Lazy loading)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=api_key)
structured_llm = llm.with_structured_output(CleanedContact)

# --- 5. ZONE DE TEST ---
col1, col2 = st.columns(2)

with col1:
    st.markdown("### üì• Input")
    raw_text = st.text_area("Donn√©e brute", height=200, placeholder="Ex: martin@airbus..com")
    run_btn = st.button("Lancer le traitement ‚ö°", type="primary")

with col2:
    st.markdown("### üì§ Output & Performance")
    
    if run_btn and raw_text:
        start_time = time.time()
        final_result = None
        step_log = []

        # --- √âTAPE 1 : REGEX (The Gatekeeper) ---
        step_log.append("1Ô∏è‚É£ Regex Check...")
        if not quick_validate_email(raw_text):
            # REJET IMM√âDIAT
            end_time = time.time()
            duration = (end_time - start_time) * 1000
            st.error(f"‚ùå Rejet√© par le Niveau 1 (Pas d'email d√©tect√©). Temps: {duration:.2f}ms")
            st.stop()
        
        # --- √âTAPE 2 : CACHE (The Memory) ---
        step_log.append("2Ô∏è‚É£ Cache Check...")
        cached_result = check_cache(raw_text)
        
        if cached_result:
            # HIT CACHE
            final_result = cached_result
            final_result['processing_source'] = "CACHE (Redis)"
            step_log.append("‚úÖ Trouv√© en cache !")
        else:
            # --- √âTAPE 3 : IA (The Brain) ---
            step_log.append("3Ô∏è‚É£ AI Processing (GPT-4o-mini)...")
            try:
                system_prompt = "Tu es ShieldFlow. Nettoie cette donn√©e B2B. Sois pr√©cis."
                prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", raw_text)])
                chain = prompt | structured_llm
                
                res = chain.invoke({})
                final_result = res.dict()
                final_result['processing_source'] = "AI (Generative)"
                
                # Mise en cache pour la prochaine fois
                save_to_cache(raw_text, final_result)
                
            except Exception as e:
                st.error(f"Erreur IA: {e}")
                st.stop()

        # --- R√âSULTATS ---
        end_time = time.time()
        total_duration = (end_time - start_time) * 1000 # en ms
        
        # Affichage du Chrono
        if total_duration < 500:
            st.success(f"‚è±Ô∏è Temps Total : **{total_duration:.0f} ms** (Ultra-Rapide)")
        elif total_duration < 1500:
            st.warning(f"‚è±Ô∏è Temps Total : **{total_duration:.0f} ms** (Standard IA)")
        else:
            st.error(f"‚è±Ô∏è Temps Total : **{total_duration:.0f} ms** (Lent)")

        # Affichage des √©tapes
        st.caption(" > ".join(step_log))
        
        # Affichage JSON
        st.json(final_result)